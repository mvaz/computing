{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A crash course on IPython (jupyter) notebooks\n",
    "\n",
    "This file contains a tutorial on analyzing the music data using Spark.\n",
    "But first, a primer on how to use the notebook.\n",
    "\n",
    "## A notebook is divided into cells\n",
    "\n",
    "The boxes labeled with `In [ ]:` are called cells.  They contain Python code.\n",
    "To run the code in the cell, click on the cell and press **Shift + Enter**\n",
    "\n",
    "Try it now on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an output like `Out[1]: 2`.\n",
    "\n",
    "Next, edit the above cell to say `1 + 2` and run it.\n",
    "\n",
    "## Check that the SparkContext is loaded\n",
    "\n",
    "Since you launched the notebook with PySpark, an object called `sc` is automatically loaded.\n",
    "This object is the connection to the Spark cluster.\n",
    "Run the cell below.\n",
    "You *should* see an output like\n",
    "\n",
    "`<pyspark.context.SparkContext at  `  *some numbers*  `  >`\n",
    "\n",
    "Otherwise, it won't be possible to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f210455f950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: How to reset the Spark cluster\n",
    "\n",
    "If you run into strange errors, you might have to reset the cluster.\n",
    " 1. In the ssh session go to `screen -r nb` and press Control + C, then Y, Enter to stop the notebook\n",
    " 2. Follow the instructions in the setup to restart the Spark cluster and launch the notebook\n",
    " 3. Refresh this page and run everything from the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music recommendations with PySpark\n",
    "\n",
    "## 1. Load some modules (libraries)\n",
    "\n",
    "The following cell imports the modules we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the artist info\n",
    "\n",
    "We will load the data in `/root/data/artist_data.txt` into Python itself as a *dictionary*\n",
    "(a hash table of key-value pairs).\n",
    "This allows us to make sense of the numeric IDs we will get in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1134999\\t06Crazy Life',\n",
       " '6821360\\tPang Nakarin',\n",
       " '10113088\\tTerfel, Bartoli- Mozart: Don',\n",
       " '10151459\\tThe Flaming Sidebur',\n",
       " '6826647\\tBodenstandig 3000']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the lines of the text into a list of strings\n",
    "f = open('/root/data/artist_data.txt', 'r')\n",
    "txt = f.read().split('\\n')\n",
    "f.close\n",
    "txt[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process each line to a key-value pair\n",
    "artist_ids = dict()\n",
    "for line in txt:\n",
    "    split = line.split('\\t')\n",
    "    if len(split) > 1:\n",
    "        artist_ids[int(split[0])] = split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Staple Singers', 'Hi-Tek feat. Buckshot')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some IDs\n",
    "artist_ids[1291], artist_ids[1989]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are that some of your favorite artists are in the list.\n",
    "To find them, make another dict() which enables reverse search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the reverse search dict\n",
    "artist_to_id = dict((v,k) for k,v in artist_ids.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000113"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter your favorite artist below!\n",
    "artist_to_id['The Beatles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the data into hadoop (in ssh)\n",
    "\n",
    "Go back to your ssh session.  We need to copy the `user_artist_data.txt` into the HDFS (Hadoop file system).\n",
    "\n",
    "Make sure you are detached from any screen with Control + A, D.  Then create a new screen and go to the hadoop folder\n",
    "```\n",
    "screen -S hdfs\n",
    "cd /root/ephemeral-hdfs/bin\n",
    "```\n",
    "\n",
    "Copy the local file into hadoop\n",
    "```\n",
    "./hadoop fs -put /root/data/user-artist-data.txt data.txt\n",
    "```\n",
    "\n",
    "Check that you copied the file:\n",
    "```\n",
    "./hadoop fs -ls\n",
    "```\n",
    "You should see the file listed as `/user/root/data.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the file into Spark\n",
    "\n",
    "The following code creates a RDD (resilient distributed dataset) with the raw strings from `data.txt`.\n",
    "The number `4` indicates the number of partitions (chunks).\n",
    "You can change the number of partitions and see if you get a performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdata = sc.textFile('data.txt', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what was loaded.  The `takeSample` command allows you to peek at random contents of an RDD object.\n",
    "\n",
    "This might take a while!  To occupy the time, switch to the tab `XX.XX.XX.XXX:8080` which shows the cluster status.  There should be one running application: PySparkShell.  Click on the name to see what is going on inside Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawlines = rawdata.takeSample(True, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it take so long?  Actually, the text data was not loaded until you used the command `takeSample`.  Spark uses lazy evaluation.  Let's run it again and note the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247.87838888168335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "rawlines = rawdata.takeSample(True, 5)\n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark includes a `cache()` command which caches the result of a computation.  Run the below code, then get timing information.  Does the speed improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data.txt MappedRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caches the result of the textFile command (?)\n",
    "rawdata.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the number of records in the RDD.  The one() function will be used to map every record to the number 1.  Then the add() function will be used to sum up all of the ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24296858"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one(x):\n",
    "    return 1\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "rawdata.map(one).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Write and run a command in the empty line below to find the total number of characters in the raw data.\n",
    "\n",
    "*Hint:* `len(x)` returns the number of characters in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll down for the answer\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANSWER TO EXERCISE:\n",
    "rawdata.map(len).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process the raw data\n",
    "\n",
    "The raw text data is not very useful to us.\n",
    "We need to process the data into relevant (key, value) pairs.\n",
    "What should we set as the key, and what should we set as the value?\n",
    "This depends on the application at hand.\n",
    "\n",
    "For now, let's narrow our focus to the artist and the counts while ignoring the users.\n",
    "The following function will convert a raw line into a (key, value) pair with the key being an integer artist ID and the value being the integer count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_to_artist_count(line):\n",
    "    line = str(line)\n",
    "    parts = line.split(' ')\n",
    "    if len(parts) != 3:\n",
    "        return (-1, 0) # a (k, v) pair indicating error\n",
    "    key = int(parts[1]) # the artist ID\n",
    "    value = int(parts[2]) # the count\n",
    "    return (key, value) # return the (k,v) pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it works correctly using the sampled text lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2258857 1404 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1404, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rawlines[0])\n",
    "raw_to_artist_count(rawlines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we will make a new rdd with the (key, value) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_rdd = rawdata.map(raw_to_artist_count).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results (and time it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228.10321593284607"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "sample_counts = count_rdd.takeSample(True, 5)\n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2281323, 12), (1273010, 1), (10698564, 1), (3114, 1), (1158516, 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can we find the *most popular* artist?\n",
    "First we need to combine the counts for each artist.\n",
    "The `combineByKey` function is perfect for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
