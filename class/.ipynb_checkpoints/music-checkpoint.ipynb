{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A crash course on IPython (jupyter) notebooks\n",
    "\n",
    "This file contains a tutorial on analyzing the music data using Spark.\n",
    "But first, a primer on how to use the notebook.\n",
    "\n",
    "## A notebook is divided into cells\n",
    "\n",
    "The boxes labeled with `In [ ]:` are called cells.  They contain Python code.\n",
    "To run the code in the cell, click on the cell and press **Shift + Enter**\n",
    "\n",
    "Try it now on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an output like `Out[1]: 2`.\n",
    "\n",
    "Next, edit the above cell to say `1 + 2` and run it.\n",
    "\n",
    "## Check that the SparkContext is loaded\n",
    "\n",
    "Since you launched the notebook with PySpark, an object called `sc` is automatically loaded.\n",
    "This object is the connection to the Spark cluster.\n",
    "Run the cell below.\n",
    "You *should* see an output like\n",
    "\n",
    "`<pyspark.context.SparkContext at  `  *some numbers*  `  >`\n",
    "\n",
    "Otherwise, it won't be possible to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: How to reset the Spark cluster\n",
    "\n",
    "If you run into strange errors, you might have to reset the cluster.\n",
    " 1. In the ssh session go to `screen -r nb` and press Control + C, then Y, Enter to stop the notebook\n",
    " 2. Follow the instructions in the setup to restart the Spark cluster and launch the notebook\n",
    " 3. Refresh this page and run everything from the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music recommendations with PySpark\n",
    "\n",
    "## 1. Load some modules (libraries)\n",
    "\n",
    "The following cell imports the modules we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the artist info\n",
    "\n",
    "We will load the data in `/root/data/artist_data.txt` into Python itself as a *dictionary*\n",
    "(a hash table of key-value pairs).\n",
    "This allows us to make sense of the numeric IDs we will get in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the lines of the text into a list of strings\n",
    "f = open('/root/data/artist_data.txt', 'r')\n",
    "txt = f.read().split('\\n')\n",
    "f.close\n",
    "txt[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process each line to a key-value pair\n",
    "artist_ids = dict()\n",
    "for line in txt:\n",
    "    split = line.split('\\t')\n",
    "    if len(split) > 1:\n",
    "        artist_ids[int(split[0])] = split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check some IDs\n",
    "artist_ids[1291], artist_ids[1989]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are that some of your favorite artists are in the list.\n",
    "To find them, make another dict() which enables reverse search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the reverse search dict\n",
    "artist_to_id = dict((v,k) for k,v in artist_ids.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enter your favorite artist below!\n",
    "artist_to_id['The Beatles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the data into hadoop (in ssh)\n",
    "\n",
    "Go back to your ssh session.  We need to copy the `user_artist_data.txt` into the HDFS (Hadoop file system).\n",
    "\n",
    "Make sure you are detached from any screen with Control + A, D.  Then create a new screen and go to the hadoop folder\n",
    "```\n",
    "screen -S hdfs\n",
    "cd /root/ephemeral-hdfs/bin\n",
    "```\n",
    "\n",
    "Copy the local file into hadoop\n",
    "```\n",
    "./hadoop fs -put /root/data/user-artist-data.txt data.txt\n",
    "```\n",
    "\n",
    "Check that you copied the file:\n",
    "```\n",
    "./hadoop fs -ls\n",
    "```\n",
    "You should see the file listed as `/user/root/data.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the file into Spark\n",
    "\n",
    "The following code creates a RDD (resilient distributed dataset) with the raw strings from `data.txt`.\n",
    "The number `4` indicates the number of partitions (chunks).\n",
    "You can change the number of partitions and see if you get a performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdata = sc.textFile('data.txt', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what was loaded.  The `takeSample` command allows you to peek at random contents of an RDD object.\n",
    "\n",
    "This might take a while!  To occupy the time, switch to the tab `XX.XX.XX.XXX:8080` which shows the cluster status.  There should be one running application: PySparkShell.  Click on the name to see what is going on inside Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawlines = rawdata.takeSample(True, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it take so long?  Actually, the text data was not loaded until you used the command `takeSample`.  Spark uses lazy evaluation.  Let's run it again and note the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "rawlines = rawdata.takeSample(True, 5)\n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark includes a `cache()` command which caches the result of a computation.  Run the below code, then get timing information.  Does the speed improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Caches the result of the textFile command (?)\n",
    "rawdata.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the number of records in the RDD.  The one() function will be used to map every record to the number 1.  Then the add() function will be used to sum up all of the ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one(x):\n",
    "    return 1\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "nrecords = rawdata.map(one).reduce(add)\n",
    "nrecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Write and run a command in the empty line below to find the total number of characters in the raw data.\n",
    "\n",
    "*Hint:* `len(x)` returns the number of characters in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below:\n",
    "# write ncharacters = rawdata.map(??).reduce(??)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll down for the answer\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "...\n",
    "<br/><br/><br/><br/><br/><br/>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANSWER TO EXERCISE:\n",
    "ncharacters = rawdata.map(len).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the mean number of characters per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float(ncharacters)/nrecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the above result by carrying out two MapReduce jobs: one to compute the number of records, and one to compute the total character count.\n",
    "\n",
    "But can we get the same result in *one* MapReduce cycle?\n",
    "\n",
    "This leads to a classic MadReduce pattern.\n",
    "\n",
    "Map each line `line` to a tuple `(1, len(x))`.\n",
    "The first element is the count and the second element is the average length for all the lines aggregated into that tuple.\n",
    "The reduce step computes the average of the counts in tuple `x` and tuple `y` *weighted* by their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_len(line):\n",
    "    return (1, len(line))\n",
    "\n",
    "def mean_reducer(x, y):\n",
    "    sumcount = x[0] + y[0]\n",
    "    return(x[0] + y[0], x[1]*float(x[0])/sumcount + y[1]*float(y[0])/sumcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawdata.map(count_len).reduce(mean_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process the raw data\n",
    "\n",
    "The raw text data is not very useful to us.\n",
    "We need to process the data into relevant (key, value) pairs.\n",
    "What should we set as the key, and what should we set as the value?\n",
    "This depends on the application at hand.\n",
    "\n",
    "For now, let's narrow our focus to the artist and the counts while ignoring the users.\n",
    "The following function will convert a raw line into a (key, value) pair with the key being an integer artist ID and the value being the integer count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_to_artist_count(line):\n",
    "    line = str(line)\n",
    "    parts = line.split(' ')\n",
    "    if len(parts) != 3:\n",
    "        return (-1, 0) # a (k, v) pair indicating error\n",
    "    key = int(parts[1]) # the artist ID\n",
    "    value = int(parts[2]) # the count\n",
    "    return (key, value) # return the (k,v) pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it works correctly using the sampled text lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rawlines[0])\n",
    "raw_to_artist_count(rawlines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we will make a new rdd with the (key, value) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_rdd = rawdata.map(raw_to_artist_count).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results (and time it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "sample_counts = count_rdd.takeSample(True, 5)\n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can we find the *most popular* artist?\n",
    "First we need to combine the counts for each artist.\n",
    "The `reduceByKey` function is perfect for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_count_rdd = count_rdd.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "sample_totals = artist_count_rdd.takeSample(True, 5)\n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to sort in descending order.\n",
    "The `sortBy` function allows us to sort by a numeric function of the (key, value) pairs.\n",
    "Sorting by the negative value gives us a list by count in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_negative_value(x):\n",
    "    return -x[1]\n",
    "\n",
    "artist_count_rdd = artist_count_rdd.sortBy(get_negative_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who is the most popular artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artist_count_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artist # 979 is the most popular.  Any guesses to who it is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artist_ids[979]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of converting ids all the time, we can just convert all the IDs into the strings.\n",
    "Note that the local object `artist_ids` gets exported to the Spark cluster.\n",
    "If you look at the screen session `nb`, you may be able to see messages about this object being \"broadcast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_keys(x):\n",
    "    try:\n",
    "        ans = (artist_ids[x[0]], x[1])\n",
    "    except:\n",
    "        ans = (str(x[0]), x[1])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "converted_count = artist_count_rdd.map(convert_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "converted_count.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exporting the result to a text file\n",
    "\n",
    "Now we have that we have produced a useful result, we would like to retrieve it back to our local computer.\n",
    "\n",
    "This takes several steps.\n",
    "\n",
    "  1. Convert the (key, value) pairs in the RDD to strings\n",
    "  2. Write the RDD to the hadoop file system\n",
    "  3. Copy from hadoop to local (on the master node)\n",
    "  4. SCP from the master node to our local computer\n",
    "  \n",
    "The following function formats our result by a tab-delimited strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_string(x):\n",
    "    return str(x[0]) + '\\t' + str(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_rdd = converted_count.map(to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to a file called 'top_artists.txt'.\n",
    "\n",
    "Note: if you have already done this before, clear the HDFS by the command `./hadoop fs -rmr top_artists.txt` in the screen `hdfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_rdd.saveAsTextFile('top_artists.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal go `screen -r hadoop` and type\n",
    "```\n",
    "./hadoop fs -ls top_artists.txt\n",
    "```\n",
    "You will see that `top_artists.txt` is actually a directory with files `part-00000` etc.\n",
    "\n",
    "We can copy to local plus merge all the separate parts by the command\n",
    "```\n",
    "./hadoop fs -getmerge top_artists.txt top_artists.txt\n",
    "```\n",
    "\n",
    "Check the result with `more top_artists.txt`.\n",
    "Then move it to the home directory with `mv top_artists.txt ~/`\n",
    "\n",
    "\n",
    "\n",
    "### In another terminal tab (non-ssh)\n",
    "\n",
    "Navigate to the directory where `class.pem` is stored.  Then type\n",
    "```\n",
    "scp -i class.pem root@XX.XX.XX.XXX:~/top_artists.txt .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extra challenges\n",
    "\n",
    "This concludes the tutorial.\n",
    "If you are finished early, have a look at \n",
    " -  https://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD\n",
    "\n",
    "and see if you can do the following:\n",
    "\n",
    " - Merge IDs which are equivalent according to `artist_alias.txt`\n",
    " - Find the mean and variance of the counts for each artist\n",
    " - Pick two artists and find out how many users listen to both vs. how many listen to only one of them\n",
    " - Adjust the number of partitions and use of `cache()` at each step, see how this affects performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
