{
 "metadata": {
  "name": "",
  "signature": "sha256:ddcf961656b5f935343502bedf028017a6a15972fa0b1c52aca760e9594fce1c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import subprocess\n",
      "import inspect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "<pyspark.context.SparkContext at 0x7f4a30152690>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hadoop_str = \"bash /root/ephemeral-hdfs/bin/hadoop\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Contents\n",
      " * [Pi example](Pi example)\n",
      " * [Sort exmaple](Sort example)\n",
      " * [Word count](Word count)\n",
      " * [Logistic Regression](Logistic Regression)\n",
      " * [Kmeans](Kmeans)\n",
      " * [Correlations](Correlations)\n",
      " * [ALS](ALS)\n",
      " * [Pagerank](Pagerank)\n",
      " * [Transitive closure](Transitive closure)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pi example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import random\n",
      "from operator import add\n",
      "\n",
      "partitions = 2\n",
      "n = 100000 * partitions\n",
      "\n",
      "def f(_):\n",
      "    x = random() * 2 - 1\n",
      "    y = random() * 2 - 1\n",
      "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
      "\n",
      "count = sc.parallelize(xrange(1, n + 1), partitions).map(f).reduce(add)\n",
      "print \"Pi is roughly %f\" % (4.0 * count / n)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pi is roughly 3.139160\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sort example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_data_dir = \"/root/spark/data/mllib/\"\n",
      "#os.listdir(local_data_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -ls\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 1 items\n",
        "-rw-r--r--   3 root supergroup         24 2015-02-01 17:56 /user/root/pr.txt\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"pagerank_data.txt pr.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.textFile(\"pr.txt\", 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sortedCount = lines.flatMap(lambda x: x.split(' ')) \\\n",
      "    .map(lambda x: (int(x), 1)) \\\n",
      "    .sortByKey(lambda x: x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = sortedCount.collect()\n",
      "for (num, unitcount) in output:\n",
      "    print num"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Word Count"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_data_dir = \"/root/spark/\"\n",
      "os.listdir(local_data_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "['RELEASE',\n",
        " 'LICENSE',\n",
        " 'sbin',\n",
        " 'examples',\n",
        " 'ec2',\n",
        " 'bin',\n",
        " 'README.md',\n",
        " 'python',\n",
        " 'logs',\n",
        " 'NOTICE',\n",
        " 'lib',\n",
        " 'conf',\n",
        " 'data']"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"LICENSE li.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.textFile(\"li.txt\", 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
      "      .map(lambda x: (x, 1)) \\\n",
      "      .reduceByKey(add)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = counts.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (word, count) in output:\n",
      "        print \"%s: %i\" % (word, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import namedtuple\n",
      "from math import exp\n",
      "from os.path import realpath\n",
      "import sys\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_data_dir = \"/root/spark/data/mllib/\"\n",
      "os.listdir(local_data_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "['sample_linear_regression_data.txt',\n",
        " 'lr_data.txt',\n",
        " 'sample_tree_data.csv',\n",
        " 'kmeans_data.txt',\n",
        " 'sample_binary_classification_data.txt',\n",
        " 'als',\n",
        " 'pagerank_data.txt',\n",
        " 'sample_svm_data.txt',\n",
        " 'sample_naive_bayes_data.txt',\n",
        " 'ridge-data',\n",
        " 'sample_movielens_data.txt',\n",
        " 'lr-data',\n",
        " 'sample_libsvm_data.txt']"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"lr_data.txt lr.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 10  # Number of dimensions\n",
      "def readPointBatch(iterator):\n",
      "    import numpy as np\n",
      "    strs = list(iterator)\n",
      "    matrix = np.zeros((len(strs), D + 1))\n",
      "    for i in xrange(len(strs)):\n",
      "        matrix[i] = np.fromstring(strs[i].replace(',', ' '), dtype=np.float32, sep=' ')\n",
      "    return [matrix]\n",
      "\n",
      "def gradient(matrix, w):\n",
      "    import numpy as np\n",
      "    Y = matrix[:, 0]    # point labels (first column of input file)\n",
      "    X = matrix[:, 1:]   # point coordinates\n",
      "    # For each point (x, y), compute gradient function, then sum these up\n",
      "    return ((1.0 / (1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)\n",
      "\n",
      "def add(x, y):\n",
      "    x += y\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "points = sc.textFile(\"lr.txt\").mapPartitions(readPointBatch).cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iterations = 20\n",
      "# Initialize w to a random value\n",
      "w = 2 * np.random.ranf(size=D) - 1\n",
      "print \"Initial w: \" + str(w)\n",
      "\n",
      "# Compute logistic regression gradient for a matrix of data points\n",
      "\n",
      "for i in range(iterations):\n",
      "    print \"On iteration %i\" % (i + 1)\n",
      "    w -= points.map(lambda m: gradient(m, w)).reduce(add)\n",
      "\n",
      "print \"Final w: \" + str(w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initial w: [-0.84768013 -0.67285186 -0.7033603   0.55430427 -0.17417282  0.58654542\n",
        "  0.88148919 -0.16971411 -0.85341344 -0.33329044]\n",
        "On iteration 1\n",
        "On iteration 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Final w: [ 845.8271294   838.65528837  831.73707681  597.72999422  775.72880998\n",
        "  710.99121093  619.0436173   805.05428983  856.76248439  807.37095896]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Logistic Regression (MLLib)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import exp\n",
      "import sys\n",
      "\n",
      "import numpy as np\n",
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from pyspark.mllib.classification import LogisticRegressionWithSGD"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parsePoint(line):\n",
      "    \"\"\"\n",
      "    Parse a line of text into an MLlib LabeledPoint object.\n",
      "    \"\"\"\n",
      "    values = [float(s) for s in line.split(' ')]\n",
      "    if values[0] == -1:   # Convert -1 labels to 0 for MLlib\n",
      "        values[0] = 0\n",
      "    return LabeledPoint(values[0], values[1:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "points = sc.textFile(\"lr.txt\").map(parsePoint)\n",
      "iterations = 20\n",
      "model = LogisticRegressionWithSGD.train(points, iterations)\n",
      "print \"Final weights: \" + str(model.weights)\n",
      "print \"Final intercept: \" + str(model.intercept)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Final weights: [0.536812178448,0.544614918986,0.524082610121,0.503641947066,0.530181519636,0.547551087865,0.538159756937,0.531253474157,0.540469321835,0.532779017307]\n",
        "Final intercept: 0.0\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Kmeans"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import numpy as np\n",
      "local_data_dir = \"/root/spark/data/mllib/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"kmeans_data.txt km.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parseVector(line):\n",
      "    return np.array([float(x) for x in line.split(' ')])\n",
      "\n",
      "\n",
      "def closestPoint(p, centers):\n",
      "    bestIndex = 0\n",
      "    closest = float(\"+inf\")\n",
      "    for i in range(len(centers)):\n",
      "        tempDist = np.sum((p - centers[i]) ** 2)\n",
      "        if tempDist < closest:\n",
      "            closest = tempDist\n",
      "            bestIndex = i\n",
      "    return bestIndex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.textFile(\"km.txt\")\n",
      "data = lines.map(parseVector).cache()\n",
      "K = 3\n",
      "convergeDist = 0.01\n",
      "\n",
      "kPoints = data.takeSample(False, K, 1)\n",
      "tempDist = 1.0\n",
      "\n",
      "while tempDist > convergeDist:\n",
      "    closest = data.map(\n",
      "        lambda p: (closestPoint(p, kPoints), (p, 1)))\n",
      "    pointStats = closest.reduceByKey(\n",
      "        lambda (x1, y1), (x2, y2): (x1 + x2, y1 + y2))\n",
      "    newPoints = pointStats.map(\n",
      "        lambda (x, (y, z)): (x, y / z)).collect()\n",
      "\n",
      "    tempDist = sum(np.sum((kPoints[x] - y) ** 2) for (x, y) in newPoints)\n",
      "\n",
      "    for (x, y) in newPoints:\n",
      "        kPoints[x] = y\n",
      "\n",
      "print \"Final centers: \" + str(kPoints)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Final centers: [array([ 0.05,  0.05,  0.05]), array([ 0.2,  0.2,  0.2]), array([ 9.1,  9.1,  9.1])]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Kmeans (MLlib)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.clustering import KMeans\n",
      "\n",
      "def parseVector(line):\n",
      "    return np.array([float(x) for x in line.split(' ')])\n",
      "\n",
      "lines = sc.textFile(\"km.txt\")\n",
      "data = lines.map(parseVector)\n",
      "model = KMeans.train(data, K)\n",
      "print \"Final centers: \" + str(model.clusterCenters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Final centers: [array([ 9.1,  9.1,  9.1]), array([ 0.05,  0.05,  0.05]), array([ 0.2,  0.2,  0.2])]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Correlations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## ALS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pagerank"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Transitive closure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Wrapping up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subprocess.check_output('git commit -a -m \"in python\"', shell=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'[master 3513777] in python\\n Committer: root <root@ip-172-31-19-143.us-west-2.compute.internal>\\nYour name and email address were configured automatically based\\non your username and hostname. Please check that they are accurate.\\nYou can suppress this message by setting them explicitly:\\n\\n    git config --global user.name \"Your Name\"\\n    git config --global user.email you@example.com\\n\\nAfter doing this, you may fix the identity used for this commit with:\\n\\n    git commit --amend --reset-author\\n\\n 1 file changed, 41 insertions(+), 16 deletions(-)\\n'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.stop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}