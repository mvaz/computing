{
 "metadata": {
  "name": "",
  "signature": "sha256:6c93f476b41de0222bd44c97e283cb8a89a65b04b89dcb68b1ea11804a9bd6dd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Basic examples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are basic examples taken from `/spark-1.2.0/examples/src/main/python`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import subprocess\n",
      "import inspect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "<pyspark.context.SparkContext at 0x7fc419ed2710>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hadoop_str = \"bash /root/ephemeral-hdfs/bin/hadoop\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Contents\n",
      " * [Pi example](#pi)\n",
      " * [Sort exmaple](#sort)\n",
      " * [Word count](#wc)\n",
      " * [Logistic Regression](#lr)\n",
      " * [Kmeans](#km)\n",
      " * [Correlations](#corr)\n",
      " * [ALS](#als)\n",
      " * [Pagerank](#pr)\n",
      " * [Transitive closure](#tc)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'pi'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pi example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import random\n",
      "from operator import add\n",
      "\n",
      "partitions = 2\n",
      "n = 100000 * partitions\n",
      "\n",
      "def f(_):\n",
      "    x = random() * 2 - 1\n",
      "    y = random() * 2 - 1\n",
      "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
      "\n",
      "count = sc.parallelize(xrange(1, n + 1), partitions).map(f).reduce(add)\n",
      "print \"Pi is roughly %f\" % (4.0 * count / n)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pi is roughly 3.139160\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'sort'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sort example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -ls\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 1 items\n",
        "-rw-r--r--   3 root supergroup         24 2015-02-01 17:56 /user/root/pr.txt\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_data_dir = \"/root/spark/data/mllib/\"\n",
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"pagerank_data.txt pr.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.textFile(\"pr.txt\", 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sortedCount = lines.flatMap(lambda x: x.split(' ')) \\\n",
      "    .map(lambda x: (int(x), 1)) \\\n",
      "    .sortByKey(lambda x: x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = sortedCount.collect()\n",
      "for (num, unitcount) in output:\n",
      "    print num"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'wc'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Word Count"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_data_dir = \"/root/spark/\"\n",
      "os.listdir(local_data_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "['RELEASE',\n",
        " 'LICENSE',\n",
        " 'sbin',\n",
        " 'examples',\n",
        " 'ec2',\n",
        " 'bin',\n",
        " 'README.md',\n",
        " 'python',\n",
        " 'logs',\n",
        " 'NOTICE',\n",
        " 'lib',\n",
        " 'conf',\n",
        " 'data']"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"LICENSE li.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.textFile(\"li.txt\", 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
      "      .map(lambda x: (x, 1)) \\\n",
      "      .reduceByKey(add)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = counts.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (word, count) in output:\n",
      "        print \"%s: %i\" % (word, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'lr'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import namedtuple\n",
      "from math import exp\n",
      "from os.path import realpath\n",
      "import sys\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_data_dir = \"/root/spark/data/mllib/\"\n",
      "os.listdir(local_data_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "['sample_linear_regression_data.txt',\n",
        " 'lr_data.txt',\n",
        " 'sample_tree_data.csv',\n",
        " 'kmeans_data.txt',\n",
        " 'sample_binary_classification_data.txt',\n",
        " 'als',\n",
        " 'pagerank_data.txt',\n",
        " 'sample_svm_data.txt',\n",
        " 'sample_naive_bayes_data.txt',\n",
        " 'ridge-data',\n",
        " 'sample_movielens_data.txt',\n",
        " 'lr-data',\n",
        " 'sample_libsvm_data.txt']"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"lr_data.txt lr.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 10  # Number of dimensions\n",
      "def readPointBatch(iterator):\n",
      "    import numpy as np\n",
      "    strs = list(iterator)\n",
      "    matrix = np.zeros((len(strs), D + 1))\n",
      "    for i in xrange(len(strs)):\n",
      "        matrix[i] = np.fromstring(strs[i].replace(',', ' '), dtype=np.float32, sep=' ')\n",
      "    return [matrix]\n",
      "\n",
      "def gradient(matrix, w):\n",
      "    import numpy as np\n",
      "    Y = matrix[:, 0]    # point labels (first column of input file)\n",
      "    X = matrix[:, 1:]   # point coordinates\n",
      "    # For each point (x, y), compute gradient function, then sum these up\n",
      "    return ((1.0 / (1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)\n",
      "\n",
      "def add(x, y):\n",
      "    x += y\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "points = sc.textFile(\"lr.txt\").mapPartitions(readPointBatch).cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iterations = 20\n",
      "# Initialize w to a random value\n",
      "w = 2 * np.random.ranf(size=D) - 1\n",
      "print \"Initial w: \" + str(w)\n",
      "\n",
      "# Compute logistic regression gradient for a matrix of data points\n",
      "\n",
      "for i in range(iterations):\n",
      "    print \"On iteration %i\" % (i + 1)\n",
      "    w -= points.map(lambda m: gradient(m, w)).reduce(add)\n",
      "\n",
      "print \"Final w: \" + str(w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initial w: [-0.84768013 -0.67285186 -0.7033603   0.55430427 -0.17417282  0.58654542\n",
        "  0.88148919 -0.16971411 -0.85341344 -0.33329044]\n",
        "On iteration 1\n",
        "On iteration 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "On iteration 20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Final w: [ 845.8271294   838.65528837  831.73707681  597.72999422  775.72880998\n",
        "  710.99121093  619.0436173   805.05428983  856.76248439  807.37095896]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Logistic Regression (MLLib)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import exp\n",
      "import sys\n",
      "\n",
      "import numpy as np\n",
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from pyspark.mllib.classification import LogisticRegressionWithSGD"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parsePoint(line):\n",
      "    \"\"\"\n",
      "    Parse a line of text into an MLlib LabeledPoint object.\n",
      "    \"\"\"\n",
      "    values = [float(s) for s in line.split(' ')]\n",
      "    if values[0] == -1:   # Convert -1 labels to 0 for MLlib\n",
      "        values[0] = 0\n",
      "    return LabeledPoint(values[0], values[1:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "points = sc.textFile(\"lr.txt\").map(parsePoint)\n",
      "iterations = 20\n",
      "model = LogisticRegressionWithSGD.train(points, iterations)\n",
      "print \"Final weights: \" + str(model.weights)\n",
      "print \"Final intercept: \" + str(model.intercept)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Final weights: [0.536812178448,0.544614918986,0.524082610121,0.503641947066,0.530181519636,0.547551087865,0.538159756937,0.531253474157,0.540469321835,0.532779017307]\n",
        "Final intercept: 0.0\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'km'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Kmeans"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import numpy as np\n",
      "local_data_dir = \"/root/spark/data/mllib/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"kmeans_data.txt km.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parseVector(line):\n",
      "    return np.array([float(x) for x in line.split(' ')])\n",
      "\n",
      "\n",
      "def closestPoint(p, centers):\n",
      "    bestIndex = 0\n",
      "    closest = float(\"+inf\")\n",
      "    for i in range(len(centers)):\n",
      "        tempDist = np.sum((p - centers[i]) ** 2)\n",
      "        if tempDist < closest:\n",
      "            closest = tempDist\n",
      "            bestIndex = i\n",
      "    return bestIndex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.textFile(\"km.txt\")\n",
      "data = lines.map(parseVector).cache()\n",
      "K = 3\n",
      "convergeDist = 0.01\n",
      "\n",
      "kPoints = data.takeSample(False, K, 1)\n",
      "tempDist = 1.0\n",
      "\n",
      "while tempDist > convergeDist:\n",
      "    closest = data.map(\n",
      "        lambda p: (closestPoint(p, kPoints), (p, 1)))\n",
      "    pointStats = closest.reduceByKey(\n",
      "        lambda (x1, y1), (x2, y2): (x1 + x2, y1 + y2))\n",
      "    newPoints = pointStats.map(\n",
      "        lambda (x, (y, z)): (x, y / z)).collect()\n",
      "\n",
      "    tempDist = sum(np.sum((kPoints[x] - y) ** 2) for (x, y) in newPoints)\n",
      "\n",
      "    for (x, y) in newPoints:\n",
      "        kPoints[x] = y\n",
      "\n",
      "print \"Final centers: \" + str(kPoints)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Final centers: [array([ 0.05,  0.05,  0.05]), array([ 0.2,  0.2,  0.2]), array([ 9.1,  9.1,  9.1])]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Kmeans (MLlib)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.clustering import KMeans\n",
      "\n",
      "def parseVector(line):\n",
      "    return np.array([float(x) for x in line.split(' ')])\n",
      "\n",
      "lines = sc.textFile(\"km.txt\")\n",
      "data = lines.map(parseVector)\n",
      "model = KMeans.train(data, K)\n",
      "print \"Final centers: \" + str(model.clusterCenters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Final centers: [array([ 9.1,  9.1,  9.1]), array([ 0.05,  0.05,  0.05]), array([ 0.2,  0.2,  0.2])]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'corr'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Correlations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filepath = '/root/spark/data/mllib/sample_linear_regression_data.txt'\n",
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+filepath+\" slrd.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from pyspark.mllib.stat import Statistics\n",
      "from pyspark.mllib.util import MLUtils"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corrType = 'pearson'\n",
      "points = MLUtils.loadLibSVMFile(sc, \"slrd.txt\")\\\n",
      "        .map(lambda lp: LabeledPoint(lp.label, lp.features.toArray()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print\n",
      "print 'Summary of data file: ' + filepath\n",
      "print '%d data points' % points.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Summary of data file: /root/spark/data/mllib/sample_linear_regression_data.txt\n",
        "501 data points"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Statistics (correlations)\n",
      "print\n",
      "print 'Correlation (%s) between label and each feature' % corrType\n",
      "print 'Feature\\tCorrelation'\n",
      "numFeatures = points.take(1)[0].features.size\n",
      "labelRDD = points.map(lambda lp: lp.label)\n",
      "for i in range(numFeatures):\n",
      "    featureRDD = points.map(lambda lp: lp.features[i])\n",
      "    corr = Statistics.corr(labelRDD, featureRDD, corrType)\n",
      "    print '%d\\t%g' % (i, corr)\n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Correlation (pearson) between label and each feature\n",
        "Feature\tCorrelation\n",
        "0\t0.00595645"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1\t0.0332056"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2\t-0.0406646"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3\t0.123178"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4\t0.0240118"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5\t0.0648617"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6\t-0.0223995"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7\t-0.0279813"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8\t-0.0359889"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9\t0.0345207"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'als'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## ALS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "\n",
      "import numpy as np\n",
      "from numpy.random import rand\n",
      "from numpy import matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rmse(R, ms, us):\n",
      "    diff = R - ms * us.T\n",
      "    return np.sqrt(np.sum(np.power(diff, 2)) / M * U)\n",
      "\n",
      "\n",
      "def update(i, vec, mat, ratings):\n",
      "    uu = mat.shape[0]\n",
      "    ff = mat.shape[1]\n",
      "\n",
      "    XtX = mat.T * mat\n",
      "    Xty = mat.T * ratings[i, :].T\n",
      "\n",
      "    for j in range(ff):\n",
      "        XtX[j, j] += LAMBDA * uu\n",
      "\n",
      "    return np.linalg.solve(XtX, Xty)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LAMBDA = 0.01   # regularization\n",
      "np.random.seed(42)\n",
      "\n",
      "\"\"\"WARN: This is a naive implementation of ALS and is given as an\n",
      "  example. Please use the ALS method found in pyspark.mllib.recommendation for more\n",
      "  conventional use.\"\"\"\n",
      "\n",
      "M = 100\n",
      "U = 500\n",
      "F = 10\n",
      "ITERATIONS = 5\n",
      "partitions = 2\n",
      "\n",
      "print \"Running ALS with M=%d, U=%d, F=%d, iters=%d, partitions=%d\\n\" % \\\n",
      "    (M, U, F, ITERATIONS, partitions)\n",
      "\n",
      "R = matrix(rand(M, F)) * matrix(rand(U, F).T)\n",
      "ms = matrix(rand(M, F))\n",
      "us = matrix(rand(U, F))\n",
      "\n",
      "Rb = sc.broadcast(R)\n",
      "msb = sc.broadcast(ms)\n",
      "usb = sc.broadcast(us)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running ALS with M=100, U=500, F=10, iters=5, partitions=2\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(ITERATIONS):\n",
      "    ms = sc.parallelize(range(M), partitions) \\\n",
      "           .map(lambda x: update(x, msb.value[x, :], usb.value, Rb.value)) \\\n",
      "           .collect()\n",
      "    # collect() returns a list, so array ends up being\n",
      "    # a 3-d array, we take the first 2 dims for the matrix\n",
      "    ms = matrix(np.array(ms)[:, :, 0])\n",
      "    msb = sc.broadcast(ms)\n",
      "\n",
      "    us = sc.parallelize(range(U), partitions) \\\n",
      "           .map(lambda x: update(x, usb.value[x, :], msb.value, Rb.value.T)) \\\n",
      "           .collect()\n",
      "    us = matrix(np.array(us)[:, :, 0])\n",
      "    usb = sc.broadcast(us)\n",
      "\n",
      "    error = rmse(R, ms, us)\n",
      "    print \"Iteration %d:\" % i\n",
      "    print \"\\nRMSE: %5.4f\\n\" % error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration 0:\n",
        "\n",
        "RMSE: 111.4555\n",
        "\n",
        "Iteration 1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "RMSE: 36.5686\n",
        "\n",
        "Iteration 2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "RMSE: 15.8659\n",
        "\n",
        "Iteration 3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "RMSE: 15.7530\n",
        "\n",
        "Iteration 4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "RMSE: 15.7746\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'pr'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pagerank"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_data_dir = \"/root/spark/data/mllib/\"\n",
      "print(subprocess.check_output(hadoop_str + \" fs -copyFromLocal \"+local_data_dir+\"pagerank_data.txt pr.txt\", shell=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "This is an example implementation of PageRank. For more conventional use,\n",
      "Please refer to PageRank implementation provided by graphx\n",
      "\"\"\"\n",
      "\n",
      "import re\n",
      "import sys\n",
      "from operator import add\n",
      "\n",
      "from pyspark import SparkContext\n",
      "\n",
      "\n",
      "def computeContribs(urls, rank):\n",
      "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
      "    num_urls = len(urls)\n",
      "    for url in urls:\n",
      "        yield (url, rank / num_urls)\n",
      "\n",
      "\n",
      "def parseNeighbors(urls):\n",
      "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
      "    parts = re.split(r'\\s+', urls)\n",
      "    return parts[0], parts[1]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.textFile(\"pr.txt\", 1)\n",
      "iterations = 10\n",
      "\n",
      "# Loads all URLs from input file and initialize their neighbors.\n",
      "links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
      "\n",
      "# Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
      "ranks = links.map(lambda (url, neighbors): (url, 1.0))\n",
      "\n",
      "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
      "for iteration in xrange(iterations):\n",
      "    # Calculates URL contributions to the rank of other URLs.\n",
      "    contribs = links.join(ranks).flatMap(\n",
      "        lambda (url, (urls, rank)): computeContribs(urls, rank))\n",
      "\n",
      "    # Re-calculates URL ranks based on neighbor contributions.\n",
      "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
      "\n",
      "# Collects all URL ranks and dump them to console.\n",
      "for (link, rank) in ranks.collect():\n",
      "    print \"%s has rank: %s.\" % (link, rank)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 has rank: 1.73800730412.\n",
        "4 has rank: 0.753997565294.\n",
        "3 has rank: 0.753997565294.\n",
        "2 has rank: 0.753997565294.\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'tc'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Transitive closure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "from random import Random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numEdges = 200\n",
      "numVertices = 100\n",
      "rand = Random(42)\n",
      "\n",
      "\n",
      "def generateGraph():\n",
      "    edges = set()\n",
      "    while len(edges) < numEdges:\n",
      "        src = rand.randrange(0, numEdges)\n",
      "        dst = rand.randrange(0, numEdges)\n",
      "        if src != dst:\n",
      "            edges.add((src, dst))\n",
      "    return edges\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "partitions = 100\n",
      "tc = sc.parallelize(generateGraph(), partitions).cache()\n",
      "\n",
      "# Linear transitive closure: each round grows paths by one edge,\n",
      "# by joining the graph's edges with the already-discovered paths.\n",
      "# e.g. join the path (y, z) from the TC with the edge (x, y) from\n",
      "# the graph to obtain the path (x, z).\n",
      "\n",
      "# Because join() joins on keys, the edges are stored in reversed order.\n",
      "edges = tc.map(lambda (x, y): (y, x))\n",
      "\n",
      "oldCount = 0L\n",
      "nextCount = tc.count()\n",
      "while True:\n",
      "    oldCount = nextCount\n",
      "    # Perform the join, obtaining an RDD of (y, (z, x)) pairs,\n",
      "    # then project the result to obtain the new (x, z) paths.\n",
      "    new_edges = tc.join(edges).map(lambda (_, (a, b)): (b, a))\n",
      "    tc = tc.union(new_edges).distinct().cache()\n",
      "    nextCount = tc.count()\n",
      "    if nextCount == oldCount:\n",
      "        break\n",
      "\n",
      "print \"TC has %i edges\" % tc.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Wrapping up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subprocess.check_output('git commit -a -m \"in python\"', shell=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'[master 3513777] in python\\n Committer: root <root@ip-172-31-19-143.us-west-2.compute.internal>\\nYour name and email address were configured automatically based\\non your username and hostname. Please check that they are accurate.\\nYou can suppress this message by setting them explicitly:\\n\\n    git config --global user.name \"Your Name\"\\n    git config --global user.email you@example.com\\n\\nAfter doing this, you may fix the identity used for this commit with:\\n\\n    git commit --amend --reset-author\\n\\n 1 file changed, 41 insertions(+), 16 deletions(-)\\n'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.stop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}